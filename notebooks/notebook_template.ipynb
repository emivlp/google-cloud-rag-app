{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Building a Production-Ready RAG Application on Google Cloud\n",
        "\n",
        "**_NOTE_**: This notebook has been tested in the following environment:\n",
        "* Python version = 3.12+\n",
        "* Google Cloud SDK (gcloud)\n",
        "* Docker\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook provides a comprehensive, step-by-step guide to building and deploying a sophisticated Retrieval-Augmented Generation (RAG) application on Google Cloud. The final product is a scalable, serverless API that can answer questions about a specific document, complete with a separate, interactive Streamlit UI.\n",
        "\n",
        "This guide is intended for developers and ML engineers looking to understand the end-to-end lifecycle of a production AI application, from data processing to a fully deployed, containerized web service.\n",
        "\n",
        "This notebook uses a two-part architecture:\n",
        "1.  An offline **Indexing Job** to process documents and create a vector store.\n",
        "2.  A lightweight **Serving Application** deployed on Cloud Run.\n",
        "\n",
        "### Objective\n",
        "\n",
        "In this tutorial, you will learn how to:\n",
        "-   Set up a Google Cloud project with the necessary APIs and permissions.\n",
        "-   Create a development environment using Vertex AI Workbench.\n",
        "-   Build a RAG indexing pipeline using LangChain and Vertex AI models.\n",
        "-   Persist the generated index to Google Cloud Storage.\n",
        "-   Containerize a Python web application using a Dockerfile.\n",
        "-   Deploy the application as a scalable, serverless API on Cloud Run.\n",
        "-   Create and run a separate Streamlit UI to interact with the API.\n",
        "\n",
        "This tutorial uses the following Google Cloud services:\n",
        "-   **Vertex AI** (for Gemini and Embedding Models)\n",
        "-   **Cloud Run** (for serverless deployment)\n",
        "-   **Cloud Build** (for container builds)\n",
        "-   **Artifact Registry** (for container storage)\n",
        "-   **Cloud Storage** (for data persistence)\n",
        "-   **Vertex AI Workbench** (for the development environment)\n",
        "\n",
        "\n",
        "### Dataset\n",
        "\n",
        "This project uses the seminal AI research paper **\"Attention Is All You Need\"** as its source document. The paper is publicly available and will be downloaded directly from arXiv during the indexing process. This serves as the knowledge base for our RAG agent.\n",
        "\n",
        "### Costs 💵\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "* [Vertex AI](https://cloud.google.com/vertex-ai/pricing)\n",
        "* [Cloud Run](https://cloud.google.com/run/pricing)\n",
        "* [Cloud Storage](https://cloud.google.com/storage/pricing)\n",
        "* [Cloud Build](https://cloud.google.com/build/pricing)\n",
        "* [Artifact Registry](https://cloud.google.com/artifact-registry/pricing)\n",
        "\n",
        "To generate a cost estimate based on your projected usage, use the [Pricing Calculator](https://cloud.google.com/products/calculator/). Remember to clean up all resources after completing the tutorial to avoid incurring further charges.\n",
        "\n",
        "---\n",
        "## Before you begin\n",
        "\n",
        "### 1. Set up your Google Cloud project\n",
        "Follow these steps to set up your environment.\n",
        "1.  [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager).\n",
        "2.  [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "3.  [Enable all necessary APIs](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,run.googleapis.com,cloudbuild.googleapis.com,artifactregistry.googleapis.com,storage.googleapis.com,compute.googleapis.com,notebooks.googleapis.com).\n",
        "4.  Install and initialize the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "#### Set your project ID and Region\n",
        "Update the following variables with your specific project details."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- YOUR CONFIGURATION ---\n",
        "PROJECT_ID = \"new-rag-project-prod\"  # @param {type:\"string\"}\n",
        "REGION = \"europe-west1\"  # @param {type:\"string\"}\n",
        "\n",
        "# --- SET GCLOUD DEFAULTS ---\n",
        "!gcloud config set project {PROJECT_ID}\n",
        "!gcloud config set run/region {REGION}"
      ],
      "metadata": {
        "id": "ntAk7QmoWICF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Set up your Development Environment\n",
        "For this project, it is highly recommended to use a **Vertex AI Workbench** instance to avoid resource limitations and dependency conflicts.\n",
        "\n",
        "1.  [Create a new Vertex AI Workbench instance](https://console.cloud.google.com/vertex-ai/workbench/instances) in the same `REGION` as above.\n",
        "2.  Once it's running, click **\"OPEN JUPYTERLAB\"** and open a new **Terminal**.\n",
        "3.  All subsequent commands in this guide should be run from that Workbench terminal.\n",
        "\n",
        "---\n",
        "## Part 1: The Indexing Job ⚙️\n",
        "\n",
        "This is a one-time process to create the RAG agent's \"brain.\"\n",
        "\n",
        "### 1. Create the project files\n",
        "First, we create the directory and the necessary Python script and dependency file."
      ],
      "metadata": {
        "id": "IhUjgJb_WKLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Create the project directory\n",
        "mkdir -p ~/rag-project\n",
        "cd ~/rag-project\n",
        "\n",
        "# Create requirements.txt for the build script\n",
        "cat > requirements.txt << EOF\n",
        "langchain-google-vertexai\n",
        "langchain\n",
        "scikit-learn\n",
        "unstructured[pdf]\n",
        "pypdf\n",
        "requests\n",
        "numpy\n",
        "EOF\n",
        "\n",
        "# Create the build_index.py script\n",
        "cat > build_index.py << EOF\n",
        "import pickle\n",
        "import requests\n",
        "import vertexai\n",
        "import numpy as np\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_google_vertexai import VertexAIEmbeddings, VertexAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "print(\"--- Starting Index Build Job ---\")\n",
        "\n",
        "PROJECT_ID = \"$(gcloud config get-value project)\"\n",
        "REGION = \"europe-west1\"\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "texts = [d.page_content for d in PyPDFLoader(\"https://arxiv.org/pdf/1706.03762.pdf\").load() if d.page_content.strip()]\n",
        "print(f\"Loaded {len(texts)} pages.\")\n",
        "\n",
        "summarizer = VertexAI(model_name=\"gemini-2.5-flash\")\n",
        "summaries = summarizer.batch(texts)\n",
        "print(f\"Generated {len(summaries)} summaries.\")\n",
        "\n",
        "embeddings = VertexAIEmbeddings(model_name=\"gemini-embedding-001\").embed_documents(summaries)\n",
        "print(\"Created embeddings.\")\n",
        "\n",
        "with open(\"summary_embeddings.pkl\", \"wb\") as f:\n",
        "    pickle.dump(np.array(embeddings), f)\n",
        "with open(\"original_texts.pkl\", \"wb\") as f:\n",
        "    pickle.dump(texts, f)\n",
        "\n",
        "print(\"--- Index Build Job Complete ---\")\n",
        "EOF\n",
        "\n",
        "echo \"Indexing files created successfully.\""
      ],
      "metadata": {
        "id": "95362CU3WNd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Run the Indexing Job\n",
        "Now, install the dependencies into a virtual environment and run the script."
      ],
      "metadata": {
        "id": "xhhat7NpWRzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd ~/rag-project\n",
        "\n",
        "# Create and activate a virtual environment\n",
        "python3 -m venv venv\n",
        "source venv/bin/activate\n",
        "\n",
        "# Install dependencies and run the build script\n",
        "pip install -r requirements.txt\n",
        "python build_index.py"
      ],
      "metadata": {
        "id": "9TOI0OtHWTW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Create a Cloud Storage bucket and Upload the Index\n",
        "The generated `.pkl` files are now uploaded to a GCS bucket to be used by our serving application."
      ],
      "metadata": {
        "id": "stbymdOkWU_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd ~/rag-project\n",
        "\n",
        "# Create a unique GCS bucket\n",
        "GCS_BUCKET=\"gs://rag-data-bucket-$(gcloud config get-value project)\"\n",
        "gcloud storage buckets create $GCS_BUCKET --location={REGION}\n",
        "\n",
        "# Upload the index files\n",
        "gcloud storage cp *.pkl $GCS_BUCKET/\n",
        "\n",
        "echo \"Index files uploaded to $GCS_BUCKET\""
      ],
      "metadata": {
        "id": "nDqWIAPVWXLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Part 2: The Serving API 🚀\n",
        "\n",
        "Now we create and deploy the lightweight server that will handle user queries.\n",
        "\n",
        "### 1. Create the Application Files\n",
        "In a separate directory, we create the server's code, dependencies, and `Dockerfile`."
      ],
      "metadata": {
        "id": "K-lnLxQ5WZbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Create a new directory for the server\n",
        "mkdir -p ~/rag-server\n",
        "cd ~/rag-server\n",
        "\n",
        "# Create the server's requirements.txt\n",
        "cat > requirements.txt << EOF\n",
        "langchain-google-vertexai\n",
        "langchain\n",
        "scikit-learn\n",
        "google-cloud-storage\n",
        "langchain-community\n",
        "functions-framework\n",
        "numpy\n",
        "EOF\n",
        "\n",
        "# Create the Dockerfile\n",
        "cat > Dockerfile << EOF\n",
        "FROM python:3.12-slim\n",
        "WORKDIR /app\n",
        "COPY requirements.txt .\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "COPY main.py .\n",
        "CMD exec functions-framework --target=rag_http_handler\n",
        "EOF\n",
        "\n",
        "# Create the main.py server code\n",
        "cat > main.py << EOF\n",
        "import os\n",
        "import pickle\n",
        "import functools\n",
        "from google.cloud import storage\n",
        "import numpy as np\n",
        "import vertexai\n",
        "from langchain_community.vectorstores import SKLearnVectorStore\n",
        "from langchain_google_vertexai import ChatVertexAI, VertexAIEmbeddings\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain.prompts import PromptTemplate\n",
        "import functions_framework\n",
        "from flask import jsonify\n",
        "\n",
        "PROJECT_ID = \"new-rag-project-prod\"\n",
        "REGION = \"europe-west1\"\n",
        "GCS_BUCKET = f\"rag-data-bucket-{PROJECT_ID}\"\n",
        "\n",
        "@functools.lru_cache(maxsize=1)\n",
        "def _load_rag_chain():\n",
        "    print(\"--- Cold Start: Loading pre-built index from GCS ---\")\n",
        "    vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(GCS_BUCKET)\n",
        "\n",
        "    blob_embeddings = bucket.blob(\"summary_embeddings.pkl\")\n",
        "    blob_embeddings.download_to_filename(\"/tmp/summary_embeddings.pkl\")\n",
        "    blob_texts = bucket.blob(\"original_texts.pkl\")\n",
        "    blob_texts.download_to_filename(\"/tmp/original_texts.pkl\")\n",
        "    print(\"Downloaded index files.\")\n",
        "\n",
        "    with open(\"/tmp/summary_embeddings.pkl\", \"rb\") as f:\n",
        "        embeddings = pickle.load(f)\n",
        "    with open(\"/tmp/original_texts.pkl\", \"rb\") as f:\n",
        "        texts = pickle.load(f)\n",
        "    print(\"Loaded index data into memory.\")\n",
        "\n",
        "    embedding_function = VertexAIEmbeddings(model_name=\"gemini-embedding-001\", location=REGION)\n",
        "    model = ChatVertexAI(model_name=\"gemini-2.5-pro\", temperature=0.2, location=REGION)\n",
        "\n",
        "    vectorstore = SKLearnVectorStore(embedding=embedding_function)\n",
        "    vectorstore.add_texts(texts=texts, embeddings=embeddings.tolist())\n",
        "\n",
        "    retriever = vectorstore.as_retriever()\n",
        "\n",
        "    template = \"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\"\n",
        "    prompt = PromptTemplate.from_template(template)\n",
        "    chain = (\n",
        "        RunnableParallel(context=retriever, question=RunnablePassthrough())\n",
        "        | prompt\n",
        "        | model\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    print(\"--- RAG Chain Initialized Successfully ---\")\n",
        "    return chain\n",
        "\n",
        "@functions_framework.http\n",
        "def rag_http_handler(request):\n",
        "    request_json = request.get_json(silent=True)\n",
        "    if not request_json or \"query\" not in request_json:\n",
        "        return jsonify({\"error\": \"JSON body with a 'query' key is required.\"}), 400\n",
        "    query = request_json[\"query\"]\n",
        "    try:\n",
        "        rag_chain = _load_rag_chain()\n",
        "        result = rag_chain.invoke(query)\n",
        "        return jsonify({\"response\": result}), 200\n",
        "    except Exception as e:\n",
        "        print(f\"Error during chain invocation: {e}\")\n",
        "        return jsonify({\"error\": f\"Failed to process the request: {str(e)}\"}), 500\n",
        "EOF\n",
        "\n",
        "echo \"Serving files created successfully.\""
      ],
      "metadata": {
        "id": "koAE3qPPWbB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Deploy the API to Cloud Run\n",
        "Run the deployment command from the `rag-server` directory. This will build the container using your Dockerfile and deploy it."
      ],
      "metadata": {
        "id": "S4a_DlJNWevH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd ~/rag-server\n",
        "\n",
        "gcloud run deploy rag-multimodal-api \\\n",
        "    --source . \\\n",
        "    --platform managed \\\n",
        "    --region={REGION} \\\n",
        "    --allow-unauthenticated \\\n",
        "    --memory=2Gi \\\n",
        "    --clear-base-image"
      ],
      "metadata": {
        "id": "QN9qf96MWgo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Part 3: The Interactive UI 💬\n",
        "\n",
        "This section is optional but demonstrates how to build a simple frontend to interact with your new API.\n",
        "\n",
        "### 1. Create the Streamlit App"
      ],
      "metadata": {
        "id": "coN-5vVfWiUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir -p ~/rag-ui\n",
        "cd ~/rag-ui\n",
        "\n",
        "# Create requirements.txt\n",
        "cat > requirements.txt << EOF\n",
        "streamlit\n",
        "requests\n",
        "EOF\n",
        "\n",
        "# Create app.py\n",
        "# IMPORTANT: You must manually edit this file and replace the placeholder\n",
        "# with the actual URL of your deployed Cloud Run service from the step above.\n",
        "cat > app.py << EOF\n",
        "import streamlit as st\n",
        "import requests\n",
        "import json\n",
        "\n",
        "API_URL = \"YOUR_CLOUD_RUN_SERVICE_URL_HERE\" # <-- PASTE YOUR URL HERE\n",
        "\n",
        "st.set_page_config(page_title=\"RAG Research Assistant\", page_icon=\"🤖\")\n",
        "st.title(\"🤖 RAG Research Assistant\")\n",
        "st.caption(\"Ask questions about the 'Attention Is All You Need' paper.\")\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "def get_rag_response(query):\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    data = {\"query\": query}\n",
        "    try:\n",
        "        response = requests.post(API_URL, headers=headers, data=json.dumps(data), timeout=300)\n",
        "        response.raise_for_status()\n",
        "        return response.json().get(\"response\", \"Sorry, I couldn't get a response.\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"An error occurred: {e}\"\n",
        "\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "if prompt := st.chat_input(\"What is your question?\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            response = get_rag_response(prompt)\n",
        "            st.markdown(response)\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "EOF"
      ],
      "metadata": {
        "id": "B-vblNg9WkNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Run the Streamlit UI\n",
        "Run these commands in the Workbench terminal to start the UI."
      ],
      "metadata": {
        "id": "G3hn8sE7WmUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd ~/rag-ui\n",
        "\n",
        "# Create and activate a virtual environment\n",
        "python3 -m venv ui-env\n",
        "source ui-env/bin/activate\n",
        "\n",
        "# Install dependencies and run the app\n",
        "pip install -r requirements.txt\n",
        "streamlit run app.py"
      ],
      "metadata": {
        "id": "bxulXMRAWoZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, delete the individual resources you created."
      ],
      "metadata": {
        "id": "E88ALhn9Wp3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete the Cloud Run service\n",
        "!gcloud run services delete rag-multimodal-api --region={REGION} --quiet\n",
        "\n",
        "# Delete the GCS bucket\n",
        "GCS_BUCKET=\"gs://rag-data-bucket-$(gcloud config get-value project)\"\n",
        "!gcloud storage rm -r {GCS_BUCKET} --quiet\n",
        "\n",
        "# Delete the Vertex AI Workbench instance (from the Cloud Console)\n",
        "print(\"Please delete the Vertex AI Workbench instance manually from the Google Cloud Console.\")"
      ],
      "metadata": {
        "id": "5nbhrp8zWrTv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}